{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\luisl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 304446 lines.\n",
      "Sampled 30444 lines and saved to 'reduced_movie_dialogues.txt'.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9 ]+\", \" \", text) \n",
    "    return text\n",
    "\n",
    "\n",
    "cleaned_data = {}\n",
    "\n",
    "with open('movie_titles_metadata.txt', 'r', encoding='utf-8', errors='replace') as titles, \\\n",
    "     open('movie_lines.txt', 'r', encoding='utf-8', errors='replace') as lines:\n",
    "\n",
    "    movie_titles = {}\n",
    "    for title in titles:\n",
    "        parts = title.strip().split(' +++$+++ ')\n",
    "        if len(parts) >= 5:\n",
    "            movie_id = parts[0]\n",
    "            movie_titles[movie_id] = {\n",
    "                'title': parts[1],\n",
    "                'year': parts[2],\n",
    "                'genres': parts[5].strip('[]').replace(\"'\", \"\").split(', ')\n",
    "            }\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(' +++$+++ ')\n",
    "        if len(parts) >= 5:\n",
    "            line_id = parts[0]\n",
    "            character_name = parts[3]\n",
    "            movie_id = parts[2]\n",
    "            dialogue = clean_text(parts[4])\n",
    "\n",
    "            if movie_id in movie_titles:\n",
    "                movie_data = movie_titles[movie_id]\n",
    "                cleaned_data[line_id] = {\n",
    "                    'movie': movie_data['title'],\n",
    "                    'year': movie_data['year'],\n",
    "                    'genres': movie_data['genres'],\n",
    "                    'character': character_name,\n",
    "                    'line': dialogue\n",
    "                }\n",
    "\n",
    "print(f\"Processed {len(cleaned_data)} lines.\")\n",
    "\n",
    "sample_size = int(len(cleaned_data) * 0.1)\n",
    "\n",
    "random.seed(42)  \n",
    "reduced_data = random.sample(list(cleaned_data.values()), sample_size)\n",
    "\n",
    "with open('reduced_movie_dialogues.txt', 'w', encoding='utf-8') as f:\n",
    "    for entry in reduced_data:\n",
    "        movie = entry['movie']\n",
    "        year = entry['year']\n",
    "        genres = ', '.join(entry['genres'])\n",
    "        character = entry['character']\n",
    "        line = entry['line']\n",
    "\n",
    "        f.write(f\"{movie} ({year} - [{genres}]): {character}: {line}\\n\")\n",
    "\n",
    "print(f\"Sampled {sample_size} lines and saved to 'reduced_movie_dialogues.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisl\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2' \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing function\n",
    "def tokenize_input(user_input):\n",
    "\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors='pt')\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisl\\anaconda3\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size=128):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size \n",
    "    )\n",
    "\n",
    "def create_data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "            \n",
    "train_dataset = load_dataset(\"reduced_movie_dialogues.txt\", tokenizer)\n",
    "data_collator = create_data_collator(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5441ff0a7af84872bfb1fdf6f5e0a44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1967.7473, 'train_samples_per_second': 1.678, 'train_steps_per_second': 0.053, 'train_loss': 4.04705810546875, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=104, training_loss=4.04705810546875, metrics={'train_runtime': 1967.7473, 'train_samples_per_second': 1.678, 'train_steps_per_second': 0.053, 'total_flos': 215696572416000.0, 'train_loss': 4.04705810546875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Training Paraneters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-finetuned-movie-dialogues',\n",
    "    overwrite_output_dir=True,                   \n",
    "    num_train_epochs=1,                           \n",
    "    per_device_train_batch_size=32,               \n",
    "    save_steps=999_000,                           \n",
    "    save_total_limit=2,                        \n",
    "    logging_dir='./logs',                       \n",
    "    logging_steps=190000,                            \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! I am a movie bot for AAI 520, what do you want to talk about? (\"exit\" to end conversation)\n",
      "Chatbot: Cheers!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def chat_bot():\n",
    "    conversation_history = \"\"\n",
    "    #internal_system_prompt = \"You are a movie expert who answers questions about famous movies and quotes. Be concise and informative.\"\n",
    "\n",
    "    print('Chatbot: Hi! I am a movie bot for AAI 520, what do you want to talk about? (\"exit\" to end conversation)')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input('You: ')\n",
    "            \n",
    "            if user_input.lower() == 'exit':\n",
    "                print('Chatbot: Cheers!')\n",
    "                break\n",
    "            conversation_history += f\"User: {user_input}\\n\"\n",
    "            prompt =  conversation_history\n",
    "\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            input_ids = tokenizer(prompt, return_tensors='pt', max_length=100, truncation=True, padding='max_length')['input_ids']\n",
    "\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).type(torch.long)\n",
    "\n",
    "            output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            top_p=0.85,  \n",
    "            temperature=0.3, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=True\n",
    ")\n",
    "\n",
    "            \n",
    "\n",
    "            # Decode model response\n",
    "            chatbot_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Append response to conversation history and print it\n",
    "            conversation_history += f\"Chatbot: {chatbot_response}\\n\"\n",
    "            print(f\"Chatbot: {chatbot_response}\")\n",
    "\n",
    "            # Limit conversation history to the last 300 characters to avoid overload\n",
    "            conversation_history = conversation_history[-300:]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred: {e}')\n",
    "\n",
    "chat_bot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://realpython.com/build-a-chatbot-python-chatterbot/\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "NLP with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
